# -*- coding: utf-8 -*-
"""optimizing agricultural technique

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Tlga-_WrUJqc6uSzg5dKMWxCUl6WmdzN
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns 
import sklearn.cluster
import streamlit as st
from ipywidgets import interact


data = pd.read_csv("E:/Downloads/optimalcrop/agricultural project dataset.csv")

#lets check the avearage requirenment of different factors for all the crops
print("Average ratio of nitrogenin the soil:{0:2f}".format(data['N'].mean()))
print("Average ratio of phosphorous the soil:{0:2f}".format(data['P'].mean()))
print("Average ratio of potassium the soil:{0:2f}".format(data['K'].mean()))
print("Average temperature in celcius:{0:2f}".format(data['temperature'].mean()))
print("Average relative humidity in %:{0:2f}".format(data['humidity'].mean()))
print("Average ph value of the soil:{0:2f}".format(data['ph'].mean()))
print("Average rainfall in mm:{0:2f}".format(data['rainfall'].mean()))

@interact
def summary(crops = list(data['label'].value_counts().index)):
    x = data[data['label']== crops]
    z = data.drop(['label'],axis=1)
    y = []
    y.append(z)
    for i in z:
        print('Minimum ', i, 'required', x[i].min())
        print('Average', i, 'required', x[i].mean())
        print('Maximum', i, 'required', x[i].max())
        print('--------------------------------------------------')

#compare the average requirement for each crops with average conditions
@interact
def compare(crops = list(data['label'].value_counts().index)):
    x = data[data['label']== crops]
    z = data.drop(['label'],axis=1)
    y = []
    y.append(z)
    for i in z:
      print('Average', i, 'required', x[i].mean())
    print('--------------------------------------------------')

@interact
def compare(conditions=['N','K','P','rainfall','temperature','ph','humidity']):
  print("crops which require greater than average ",conditions,'\n')
  print(data[data[conditions]>data[conditions].mean()]['label'].unique())
  print('--------------------------------------------------')
  print("crops which require less than average ",conditions,'\n')
  print(data[data[conditions]<data[conditions].mean()]['label'].unique())

data.hist(figsize=(12,12), layout=(4,4), bins=20) 
#observation:
#P is right skewed
#Humidity left skewed

#finding out some important patterns
#Important observation
#we can see mothbeens grow in very low and very high pH conditions so pH is not a major factor for mothbeens
print('crops which requires very high ratio of Nitrogen content in soil:', data[data['N']>120]['label'].unique())
print('crops which requires very high ratio of Phosphorous content in soil:', data[data['P']>100]['label'].unique())
print('crops which requires very high ratio of Potassium content in soil:', data[data['K']>200]['label'].unique())
print('crops which requires very high rainfall:', data[data['rainfall']>200]['label'].unique())
print('crops which requires very low temperature:', data[data['temperature']<10]['label'].unique())
print('crops which requires very high temperature:', data[data['temperature']>40]['label'].unique())
print('crops which requires very low humidity:', data[data['humidity']<20]['label'].unique())
print('crops which requires very low ph:', data[data['ph']<4]['label'].unique())
print('crops which requires very high ph:', data[data['ph']>9]['label'].unique())

#lets find the crops best for specific seasons
print(" summer crops",data[data['temperature']>30]['label'].unique())
print("winter crops",data[data['temperature']<20]['label'].unique())
print("rainy crops ",data[data['rainfall']>200]['label'].unique())

from sklearn.cluster import KMeans
#removing the label columns as for kmeans clustering meathod we don't require labels
x=data.drop(['label'],axis=1)
#selecting all the values of data
x=x.values
#checking the shape
x.shape

#Determine Optimum number of cluster by elbow method
from sklearn.cluster import KMeans
plt.rcParams['figure.figsize'] = (10,4)
wcss = [] #An empty list is created to store the Within-Cluster-Sum-of-Squares (WCSS) values for each number of clusters.
for i in range (1,11):
    km = KMeans(n_clusters =i, init= 'k-means++', max_iter=300, n_init=10, random_state=0)
    km.fit(x)
    wcss.append(km.inertia_)   #The inertia_ attribute of the KMeans object is used to calculate the WCSS value for each value of 'i', which is then appended to the list 'wcss'.
#plot the results
plt.plot(range(1,11), wcss)
plt.title('Elbow Method', fontsize= 15)
plt.xlabel('No. of cluster')
plt.ylabel('wcss')
plt.show()

#OBSERVATION
#we get two elbows at 3 and 4. As per elbow method definition we take the last one so our no. of cluster will be #4

km = KMeans(n_clusters =4, init= 'k-means++', max_iter=300, n_init=10, random_state=0)
y_means= km.fit_predict(x)
a = data['label']
y_means = pd.DataFrame(y_means)
w = pd.concat([y_means, a], axis=1)
w =w.rename(columns= {0:'cluster'})

#After performing K-means clustering on the dataset 'x' with 4 clusters and assigning each data point to its respective cluster using the
# 'fit_predict' method, the code creates a new DataFrame 'w' by concatenating the cluster assignments (stored in 'y_means') with the original 'label' column
# from the 'data' DataFrame. The 'axis=1' argument is used to concatenate the two DataFrames along the columns.

for i in range(0,4): #for 4 clusters 0,1,2,3
    print('Crops is cluster', i, w[w['cluster']==i]['label'].unique())
    print('---------------------------------------------------------------------------------------')

#splitting the dataset x contains all the crops as well as the 7factors affecting the crops and y just contains all the labels that is the crops this is done to maintain the accuracy of the model 
y=data['label']
x=data.drop(['label'],axis=1)
print("shape of x",x.shape)
print("shape of y",y.shape)

#creating training and testing set splitting the dataset in the ratio of 80 and 20 where 80 percent data goes fpr training set and 20 percent for test set 
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=0)
print("shape of x train",x_train.shape)
print("shape of x test",x_test.shape)
print("shape of y train",y_train.shape)
print("shape of y test",y_test.shape)

#creating a predictive model
from sklearn.linear_model import LogisticRegression
model=LogisticRegression()
model.fit(x_train,y_train) #This line trains the logistic regression model on the training data x_train and the corresponding labels y_train.
y_pred= model.predict(x_test) #This line makes predictions on the test data x_test using the trained model and assigns the predicted labels to the variable y_pred.


# Load the trained Logistic Regression model
model = LogisticRegression()
model.fit(x_train, y_train)  # Make sure x_train and y_train are defined from your previous code

# Streamlit app for crop prediction
st.title('Optimal Crop Recommendation System')
st.subheader('Adjust the values to get prediction')

# Input sliders for environmental factors
N = st.slider('Nitrogen content in soil', min_value=0, max_value=200, value=100)
P = st.slider('Phosphorus content in soil', min_value=0, max_value=150, value=50)
K = st.slider('Potassium content in soil', min_value=0, max_value=250, value=100)
temperature = st.slider('Temperature in Celsius', min_value=0, max_value=50, value=25)
humidity = st.slider('Relative humidity in %', min_value=0, max_value=100, value=50)
ph = st.slider('pH value of the soil', min_value=0.0, max_value=14.0, value=7.0)
rainfall = st.slider('Rainfall in mm', min_value=0, max_value=300, value=100)

# Predicting the crop using the Logistic Regression model
prediction = model.predict(np.array([[N, P, K, temperature, humidity, ph, rainfall]]))

st.subheader('Predicted Crop:')
st.markdown(f'<p style="font-size:30px; font-weight:bold;">{prediction[0]}</p>', unsafe_allow_html=True)

#evaluating the model performance
from sklearn.metrics import accuracy_score,confusion_matrix
plt.rcParams['figure.figsize']=(9,9)
cm=confusion_matrix(y_test,y_pred)
sns.heatmap(cm,annot=True,cmap='Wistia')
plt.title('confusion matrix for logistic regression',fontsize=12)
plt.show()

#Classification report
from sklearn.metrics import classification_report
cr= classification_report (y_test,y_pred)
print(cr)
#observation 
#As we can see that the precision and recall values for all the crops are high  this shows that the accuracy of our model is very good

data.head()

#Now we will check that our model is either making correct decision or not for this we will take the values of n p k etc for rice from the above table and ask our model to predict that which crop is best suitable to grow under such conditions
prediction=model.predict((np.array([[90,40,40,20,80,7,200]])))
print('The suggested crop for given climatic condition is',prediction)